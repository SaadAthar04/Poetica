{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFXxzljOGT4C"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "# Function to get all ghazal category pages from a poet's main page (using a fixed base URL)\n",
        "def get_ghazal_category_links(poet_url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(poet_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"âŒ Error fetching poet's page: {e}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    category_links = set()\n",
        "\n",
        "    for link in soup.find_all(\"a\", href=True):\n",
        "        if \"/ghazals\" in link[\"href\"]:  # Filtering ghazal-related links\n",
        "            full_url = link[\"href\"]\n",
        "            category_links.add(full_url)\n",
        "\n",
        "    return list(category_links)\n",
        "\n",
        "# Function to fetch ghazal links from a category page (using a fixed base URL)\n",
        "def get_ghazal_links(category_url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(category_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"âŒ Error fetching ghazal links: {e}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    ghazal_links = set()\n",
        "\n",
        "    for link in soup.find_all(\"a\", href=True):\n",
        "        if \"/ghazals/\" in link[\"href\"]:\n",
        "            full_url = link[\"href\"]\n",
        "            ghazal_links.add(full_url)\n",
        "\n",
        "    return list(ghazal_links)\n",
        "\n",
        "# Function to check if a word contains English letters\n",
        "def contains_english(word):\n",
        "    return bool(re.search(r\"[a-zA-Z]\", word))\n",
        "\n",
        "# Function to scrape ghazal text from a given URL\n",
        "def scrape_ghazal(url):\n",
        "    print(f\"ðŸ“Œ Scraping: {url}\\n\")\n",
        "\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"âŒ Error fetching {url}: {e}\\n\")\n",
        "        return \"\"\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    poetry_container = soup.find_all(\"p\")\n",
        "\n",
        "    if not poetry_container:\n",
        "        print(\"âš  No ghazal found.\\n\")\n",
        "        return \"\"\n",
        "\n",
        "    poetry_text = []\n",
        "    for p in poetry_container:\n",
        "        words = [span.get_text(strip=True) for span in p.find_all(\"span\")]\n",
        "        line = \" \".join(words)\n",
        "        if contains_english(line):\n",
        "            poetry_text.append(line)\n",
        "\n",
        "    poetry_text = poetry_text[2:] if len(poetry_text) > 2 else []\n",
        "\n",
        "    if poetry_text:\n",
        "        ghazal = \"\\n\".join(poetry_text)\n",
        "        print(ghazal + \"\\n\" + \"=\"*50 + \"\\n\")\n",
        "        return ghazal\n",
        "    else:\n",
        "        print(\"âš  No English-transliterated text found.\\n\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to write the scraped data to a text file\n",
        "def write_to_text(data, filename=\"updated_ghazals.txt\"):\n",
        "    with open(filename, mode=\"a\", encoding=\"utf-8\") as file:\n",
        "        for item in data:\n",
        "            #file.write(f\"Ghazal URL: {item[0]}\\n\")\n",
        "            file.write(f\"\\n{item[1]}\\n\")\n",
        "            #file.write(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "# Main execution\n",
        "if _name_ == \"_main_\":\n",
        "    base_url = \"https://www.rekhta.org/poets?wref=rweb\"  # Replace with the actual base URL\n",
        "    category_links = get_ghazal_category_links(base_url)\n",
        "    print(f\"âœ… Found {len(category_links)} ghazal category pages.\\n\")\n",
        "\n",
        "    all_ghazal_links = []\n",
        "    for category_link in category_links:\n",
        "        links = get_ghazal_links(category_link)\n",
        "        all_ghazal_links.extend(links)\n",
        "\n",
        "    print(f\"âœ… Found {len(all_ghazal_links)} ghazals.\\n\")\n",
        "\n",
        "    scraped_data = []\n",
        "    i = 0\n",
        "    for link in all_ghazal_links:\n",
        "        i += 1\n",
        "        print(\"Ghazal no:\", i)\n",
        "        ghazal_text = scrape_ghazal(link)\n",
        "        if ghazal_text:\n",
        "            scraped_data.append([link, ghazal_text])\n",
        "        time.sleep(2)  # Delay to prevent blocking\n",
        "\n",
        "    if scraped_data:\n",
        "        write_to_text(scraped_data)\n",
        "        print(f\"âœ… Data has been written toÂ ghazals.txt\")"
      ]
    }
  ]
}